{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44220778-f414-418c-9494-80ed799b35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use findspark to confirm pyspark works\n",
    "import findspark\n",
    "import streamlit\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e105e07-882d-499e-be1c-8c09aa115977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/06 11:36:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Import pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from collections import Counter\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.master('local[10]').config('spark.driver.memory','10g').getOrCreate()\n",
    "\n",
    "conf = SparkConf().setAppName('Project').setMaster('local')\n",
    "sparkContext = SparkContext.getOrCreate(conf=conf) #SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89897865-5f51-40f0-b7b6-54df26e3ee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------+---------+-----------+-------+-----+------+------------+---------+-----------+-------------+----------+------------+-----------+--------+----------+-----------+--------+----------+-------+\n",
      "|Area_code|                Area|Self_num_ind|Self_mean|Self_median|Num_ind| Mean|Median|Pens_num_ind|Pens_mean|Pens_median|Other_num_ind|Other_mean|Other_median|Tot_num_ind|Tot_mean|Tot_median|Tax_num_ind|Tax_mean|Tax_median|Tot_tax|\n",
      "+---------+--------------------+------------+---------+-----------+-------+-----+------+------------+---------+-----------+-------------+----------+------------+-----------+--------+----------+-----------+--------+----------+-------+\n",
      "|K02000001|      United Kingdom|        3520|    30300|      16300|  25000|36800| 26500|        8560|    19900|      16900|        14200|      7240|          60|      33000|   39400|     27200|      33000|    6850|      2620| 226000|\n",
      "|E92000001|             England|        3030|    30900|      16500|  21000|37600| 26700|        7050|    20000|      16800|        12000|      7630|          64|      27700|   40300|     27500|      27700|    7160|      2660| 198000|\n",
      "|E12000001|          North East|          93|    22300|      15000|    929|30200| 24700|         366|    18400|      16300|          508|      3840|          31|       1230|   31500|     24900|       1230|    4110|      2230|   5060|\n",
      "|E11000007|         Tyne & Wear|          35|    23200|      14800|    396|30500| 25000|         136|    18000|      16200|          197|      3800|          29|        508|   31700|     25200|        508|    4120|      2290|   2090|\n",
      "|E12000002|          North West|         303|    22900|      15700|   2690|32000| 25000|         925|    18900|      16500|         1410|      5930|          43|       3490|   34000|     25700|       3490|    4930|      2360|  17200|\n",
      "|E10000006|             Cumbria|          28|    20000|      14500|    191|30200| 24900|          87|    18900|      16500|          115|      5170|          61|        265|   32300|     25800|        265|    4190|      2370|   1110|\n",
      "|E11000001|  Greater Manchester|         111|    23600|      16100|   1030|31800| 25000|         290|    17900|      16100|          500|      5730|          36|       1290|   33600|     25500|       1290|    4810|      2350|   6190|\n",
      "|E10000017|          Lancashire|          55|    22000|      15500|    440|30500| 24200|         183|    19100|      16800|          265|      6250|          55|        600|   32900|     25200|        600|    4510|      2260|   2710|\n",
      "|E11000002|          Merseyside|          49|    22600|      16400|    503|30500| 25000|         166|    18300|      16100|          235|      4380|          29|        642|   32000|     25200|        642|    4310|      2310|   2760|\n",
      "|E12000003|Yorkshire and the...|         235|    22900|      15400|   1940|30900| 24700|         696|    18700|      16500|         1130|      5210|          43|       2550|   33000|     25300|       2550|    4590|      2300|  11700|\n",
      "|E10000023|     North Yorkshire|          42|    26700|      15900|    223|33200| 24400|         112|    21000|      17500|          168|      8230|          91|        324|   38000|     26800|        324|    6270|      2500|   2030|\n",
      "|E11000003|     South Yorkshire|          56|    21300|      15200|    478|29800| 24400|         160|    17800|      16200|          260|      4130|          30|        620|   31300|     24700|        620|    4040|      2200|   2510|\n",
      "|E11000006|      West Yorkshire|          93|    22700|      15500|    816|31200| 25200|         264|    18500|      16400|          472|      5000|          44|       1060|   32900|     25500|       1060|    4510|      2330|   4770|\n",
      "|E12000004|       East Midlands|         223|    23500|      16400|   1810|31700| 25000|         643|    18900|      16500|         1010|      5780|          47|       2380|   33800|     25800|       2380|    4850|      2370|  11500|\n",
      "|E10000007|          Derbyshire|          37|    21700|      15600|    301|31500| 24800|         126|    18700|      16500|          182|      5640|          44|        411|   33200|     25200|        411|    4650|      2250|   1910|\n",
      "|E10000018|      Leicestershire|          35|    25400|      17300|    282|33700| 26700|         105|    19600|      17000|          178|      6750|          63|        374|   36500|     27600|        374|    5570|      2640|   2080|\n",
      "|E10000019|        Lincolnshire|          37|    22100|      16400|    280|30200| 24400|         118|    18800|      16400|          162|      5350|          49|        381|   32400|     25300|        381|    4390|      2300|   1670|\n",
      "|E10000024|     Nottinghamshire|          38|    22900|      16000|    310|32400| 25100|         125|    18900|      16800|          186|      5660|          49|        419|   34200|     25800|        419|    4980|      2350|   2090|\n",
      "|E12000005|       West Midlands|         260|    22800|      16100|   2100|31600| 25100|         734|    18600|      16400|         1160|      5780|          47|       2760|   33600|     25500|       2760|    4800|      2340|  13200|\n",
      "|E10000028|       Staffordshire|          45|    21100|      15400|    339|32500| 25600|         135|    19100|      17100|          205|      6340|          51|        458|   34500|     26100|        458|    5080|      2410|   2330|\n",
      "+---------+--------------------+------------+---------+-----------+-------+-----+------+------------+---------+-----------+-------------+----------+------------+-----------+--------+----------+-----------+--------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in and assign column names\n",
    "# Income data\n",
    "Income = spark.read.csv('income_csv_cleaned_no_commas.csv',header=False)\n",
    "Income = Income.toDF('Area_code','Area','Self_num_ind','Self_mean','Self_median','Num_ind','Mean','Median','Pens_num_ind','Pens_mean',\n",
    "                     'Pens_median','Other_num_ind','Other_mean','Other_median','Tot_num_ind','Tot_mean','Tot_median','Tax_num_ind','Tax_mean','Tax_median','Tot_tax')\n",
    "\n",
    "# Drop not applicable\n",
    "Income = Income.na.drop()\n",
    "Income = Income.filter(Income.Area_code!='Not applicable')\n",
    "\n",
    "# Remove original formatting\n",
    "Income = Income.filter(Income.Area!='Region/County')\n",
    "\n",
    "# Population data\n",
    "Population = spark.read.csv('population_data.csv',header=False)\n",
    "Population = Population.toDF('Code','Name','Geography','All ages','0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15',\n",
    "                     '16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37',\n",
    "                     '38','39','40','41','42','43','44','45','46','47','48','49','50','51','52','53','54','55','56','57','58','59',\n",
    "                     '60','61','62','63','64','65','66','67','68','69','70','71','72','73','74','75','76','77','78','79','80','81','82','83','84','85','86','87','88','89','90+')\n",
    "\n",
    "# Drop not applicable\n",
    "Population = Population.na.drop()\n",
    "\n",
    "# Remove original formatting\n",
    "Population = Population.filter(Population.Code!='Code')\n",
    "\n",
    "# Income data\n",
    "Statements = spark.read.csv('Transposed_Consolidated_Income_Statements.csv',header=False)\n",
    "Statements = Statements.toDF('Company','Fina_period','State','Revenue','Cost','Gross','Other','Disp_close','Admin','Operating','Fin_out','Fin_in','Share_loss','Before_tax','Tax','Profit')\n",
    "\n",
    "# Remove original formatting\n",
    "Statements = Statements.drop(col('Company'))\n",
    "Statements = Statements.drop(col('Fina_period'))\n",
    "Statements = Statements.drop(col('State'))\n",
    "\n",
    "# Remove original formatting\n",
    "Statements = Statements.filter(Statements.Cost!='Cost of sales')\n",
    "\n",
    "Income.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c2bd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/06 11:36:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_ind , int\n",
      "Mean , int\n",
      "Median , int\n",
      "Other_num_ind , int\n",
      "Other_mean , int\n",
      "Other_median , int\n",
      "Tot_num_ind , int\n",
      "Tot_mean , int\n",
      "Tot_median , int\n",
      "Tax_num_ind , int\n",
      "Tax_mean , int\n",
      "Tax_median , int\n",
      "Tot_tax , int\n",
      "All ages , int\n",
      "16 , int\n",
      "17 , int\n",
      "18 , int\n",
      "19 , int\n",
      "20 , int\n",
      "21 , int\n",
      "22 , int\n",
      "23 , int\n",
      "24 , int\n",
      "25 , int\n",
      "26 , int\n",
      "27 , int\n",
      "28 , int\n",
      "29 , int\n",
      "30 , int\n",
      "31 , int\n",
      "32 , int\n",
      "33 , int\n",
      "34 , int\n",
      "35 , int\n",
      "36 , int\n",
      "37 , int\n",
      "38 , int\n",
      "39 , int\n",
      "40 , int\n",
      "41 , int\n",
      "42 , int\n",
      "43 , int\n",
      "44 , int\n",
      "45 , int\n",
      "46 , int\n",
      "47 , int\n",
      "48 , int\n",
      "49 , int\n",
      "50 , int\n",
      "51 , int\n",
      "52 , int\n",
      "53 , int\n",
      "54 , int\n",
      "55 , int\n",
      "56 , int\n",
      "57 , int\n",
      "58 , int\n",
      "59 , int\n",
      "60 , int\n",
      "61 , int\n",
      "62 , int\n",
      "63 , int\n",
      "64 , int\n",
      "65 , int\n",
      "66 , int\n",
      "67 , int\n",
      "68 , int\n",
      "69 , int\n",
      "70 , int\n",
      "71 , int\n",
      "72 , int\n",
      "73 , int\n",
      "74 , int\n",
      "75 , int\n",
      "76 , int\n",
      "77 , int\n",
      "78 , int\n",
      "79 , int\n",
      "80 , int\n",
      "81 , int\n",
      "82 , int\n",
      "83 , int\n",
      "84 , int\n",
      "85 , int\n",
      "86 , int\n",
      "87 , int\n",
      "88 , int\n",
      "89 , int\n",
      "90+ , int\n",
      "id , int\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Condensing the data to only what is relevant\n",
    "# Dropping self employement data\n",
    "\n",
    "# Income condensed\n",
    "Cond_Inc = Income\n",
    "\n",
    "Cond_Inc = Cond_Inc.drop(col('Self_num_ind'))\n",
    "Cond_Inc = Cond_Inc.drop(col('Self_mean'))\n",
    "Cond_Inc = Cond_Inc.drop(col('Self_median'))\n",
    "\n",
    "# Dropping pension data\n",
    "Cond_Inc = Cond_Inc.drop(col('Pens_num_ind'))\n",
    "Cond_Inc = Cond_Inc.drop(col('Pens_mean'))\n",
    "Cond_Inc = Cond_Inc.drop(col('Pens_median'))\n",
    "\n",
    "# Remove the non-applicable areas\n",
    "Cond_Inc = Cond_Inc.filter(Cond_Inc.Area_code != 'Not applicable')\n",
    "\n",
    "# Population condensed\n",
    "Cond_Pop = Population\n",
    "\n",
    "# Dropping columns for children\n",
    "for colu in range(0,16):\n",
    "    Cond_Pop = Cond_Pop.drop('',col(str(colu)))\n",
    "\n",
    "# Dropping repeated data in both population and income\n",
    "Cond_Pop = Cond_Pop.drop(col('Geography'))\n",
    "Cond_Pop = Cond_Pop.drop(col('Name'))\n",
    "\n",
    "# Remove commas from numbers\n",
    "#Cond_Pop = Cond_Pop.map(lambda x: str(x).replace(',', '') if isinstance(x, str) else x)\n",
    "\n",
    "# Join two dataframes\n",
    "Data = Cond_Inc.join(Cond_Pop,Cond_Inc.Area_code == Cond_Pop.Code,'inner')\n",
    "Data = Data.drop(col('Area_code'))\n",
    "\n",
    "# Add new primary key\n",
    "Data = Data.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Get all codes in the new dataframe\n",
    "codes = [row.Code for row in Data.select('Code').collect()]\n",
    "\n",
    "Data = Data.withColumn('Num_ind',Data['Num_ind'].cast(IntegerType()))\n",
    "\n",
    "# Temporary store for string data types with primary key\n",
    "Temp_Store = Data.select('id','Code','Area')\n",
    "Temp_Store = Temp_Store.withColumnRenamed('id', 'id1')\n",
    "\n",
    "# Drop string data type columns\n",
    "Data = Data.drop(col('Code'))\n",
    "Data = Data.drop(col('Area'))\n",
    "\n",
    "for c in Data.columns:\n",
    "    Data = Data.withColumn(c, F.col(c).cast(IntegerType()))\n",
    "\n",
    "Cast_Data = Data.join(Temp_Store,Data.id == Temp_Store.id1,'inner')\n",
    "Cast_Data = Data.drop(col('id1'))\n",
    "\n",
    "for col in Cast_Data.dtypes:\n",
    "    print(col[0]+\" , \"+col[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b9812d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Imports\\nfrom pyspark.ml.evaluation import RegressionEvaluator \\nfrom pyspark.ml.recommendation import ALS\\n\\n# Split data\\ntrain_data, test_data = Data.randomSplit([0.8, 0.2])\\n\\n# Build recommender model\\nals = ALS(maxIter=5, regParam=0.01, userCol=\\'Truster\\', itemCol=\\'Movie\\', ratingCol=\\'Rating\\') \\n\\n# Fit\\nmodel = als.fit(train_data)\\n\\n# Evaluate\\npredictions = model.transform(test_data)\\npredictions.show()\\n\\n# Drop null\\npredictions = predictions.na.drop() \\n\\n# RMSE\\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Rating\",predictionCol=\"prediction\") \\nrmse = evaluator.evaluate(predictions) \\nprint(\"Root-mean-square error = \" + str(rmse))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Imports\n",
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = Data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build recommender model\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol='Truster', itemCol='Movie', ratingCol='Rating') \n",
    "\n",
    "# Fit\n",
    "model = als.fit(train_data)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show()\n",
    "\n",
    "# Drop null\n",
    "predictions = predictions.na.drop() \n",
    "\n",
    "# RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Rating\",predictionCol=\"prediction\") \n",
    "rmse = evaluator.evaluate(predictions) \n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
